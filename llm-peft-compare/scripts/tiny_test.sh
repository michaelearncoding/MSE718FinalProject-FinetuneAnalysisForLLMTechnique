#!/bin/bash

# è¶…çº§ç²¾ç®€çš„æµ‹è¯•è„šæœ¬ï¼Œä½¿ç”¨æå°æ•°æ®é›†å’ŒæçŸ­è®­ç»ƒæ—¶é—´

echo "=================================================="
echo "ğŸ§ª è¶…ç®€åŒ–ç‰ˆæœ¬çš„å¾®è°ƒæµ‹è¯•"
echo "=================================================="

# åˆ›å»ºæµ‹è¯•æ•°æ®
python scripts/create_test_data.py

# ä¿®æ”¹è®­ç»ƒè„šæœ¬ä½¿ç”¨æµ‹è¯•æ•°æ®
TRAIN_SCRIPT="scripts/train_instruction.py"
cp "$TRAIN_SCRIPT" "${TRAIN_SCRIPT}.backup"
sed -i '' 's|TRAIN_FILE = os.path.join(PROJECT_ROOT, "data", "alpaca_train.jsonl")|TRAIN_FILE = os.path.join(PROJECT_ROOT, "data", "tiny_test.jsonl")|g' "$TRAIN_SCRIPT"

# æç®€è®­ç»ƒå‚æ•°
echo "ğŸ”„ å¼€å§‹æç®€ç‰ˆå¾®è°ƒ..."
python $TRAIN_SCRIPT --method lora --model_size tiny --batch_size 2 --epochs 1 --gradient_accumulation_steps 1

# æ¢å¤åŸå§‹è®­ç»ƒè„šæœ¬
mv "${TRAIN_SCRIPT}.backup" "$TRAIN_SCRIPT"

echo "=================================================="
echo "æµ‹è¯•å®Œæˆ"
echo "==================================================" 
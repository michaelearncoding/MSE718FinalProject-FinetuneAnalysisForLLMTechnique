{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9999230828397816,
  "eval_steps": 500,
  "global_step": 3250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003076686408737789,
      "grad_norm": 1.543134331703186,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.8754,
      "step": 10
    },
    {
      "epoch": 0.006153372817475578,
      "grad_norm": 1.6060943603515625,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.8765,
      "step": 20
    },
    {
      "epoch": 0.009230059226213368,
      "grad_norm": 1.9358795881271362,
      "learning_rate": 6e-06,
      "loss": 1.742,
      "step": 30
    },
    {
      "epoch": 0.012306745634951157,
      "grad_norm": 1.7813806533813477,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.911,
      "step": 40
    },
    {
      "epoch": 0.015383432043688947,
      "grad_norm": 1.850482702255249,
      "learning_rate": 1e-05,
      "loss": 1.8564,
      "step": 50
    },
    {
      "epoch": 0.018460118452426736,
      "grad_norm": 1.729852557182312,
      "learning_rate": 1.2e-05,
      "loss": 1.8249,
      "step": 60
    },
    {
      "epoch": 0.021536804861164525,
      "grad_norm": 1.626412272453308,
      "learning_rate": 1.4e-05,
      "loss": 1.6988,
      "step": 70
    },
    {
      "epoch": 0.024613491269902314,
      "grad_norm": 1.6978667974472046,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.6748,
      "step": 80
    },
    {
      "epoch": 0.027690177678640106,
      "grad_norm": 1.9762394428253174,
      "learning_rate": 1.8e-05,
      "loss": 1.7011,
      "step": 90
    },
    {
      "epoch": 0.030766864087377895,
      "grad_norm": 1.389716625213623,
      "learning_rate": 2e-05,
      "loss": 1.6652,
      "step": 100
    },
    {
      "epoch": 0.03384355049611568,
      "grad_norm": 1.2881301641464233,
      "learning_rate": 1.9999502669559432e-05,
      "loss": 1.5679,
      "step": 110
    },
    {
      "epoch": 0.03692023690485347,
      "grad_norm": 1.6438038349151611,
      "learning_rate": 1.9998010727705237e-05,
      "loss": 1.5556,
      "step": 120
    },
    {
      "epoch": 0.03999692331359126,
      "grad_norm": 2.0381581783294678,
      "learning_rate": 1.9995524322835035e-05,
      "loss": 1.6193,
      "step": 130
    },
    {
      "epoch": 0.04307360972232905,
      "grad_norm": 1.8062448501586914,
      "learning_rate": 1.9992043702261795e-05,
      "loss": 1.6074,
      "step": 140
    },
    {
      "epoch": 0.04615029613106684,
      "grad_norm": 1.5019804239273071,
      "learning_rate": 1.9987569212189224e-05,
      "loss": 1.525,
      "step": 150
    },
    {
      "epoch": 0.04922698253980463,
      "grad_norm": 1.7379028797149658,
      "learning_rate": 1.998210129767735e-05,
      "loss": 1.5431,
      "step": 160
    },
    {
      "epoch": 0.05230366894854242,
      "grad_norm": 1.1448992490768433,
      "learning_rate": 1.9975640502598243e-05,
      "loss": 1.5313,
      "step": 170
    },
    {
      "epoch": 0.05538035535728021,
      "grad_norm": 1.385980486869812,
      "learning_rate": 1.996818746958191e-05,
      "loss": 1.5457,
      "step": 180
    },
    {
      "epoch": 0.058457041766018,
      "grad_norm": 1.3203096389770508,
      "learning_rate": 1.9959742939952393e-05,
      "loss": 1.514,
      "step": 190
    },
    {
      "epoch": 0.06153372817475579,
      "grad_norm": 1.9119266271591187,
      "learning_rate": 1.9950307753654016e-05,
      "loss": 1.5301,
      "step": 200
    },
    {
      "epoch": 0.06461041458349358,
      "grad_norm": 1.0811259746551514,
      "learning_rate": 1.9939882849167853e-05,
      "loss": 1.5315,
      "step": 210
    },
    {
      "epoch": 0.06768710099223137,
      "grad_norm": 1.5618265867233276,
      "learning_rate": 1.9928469263418376e-05,
      "loss": 1.5633,
      "step": 220
    },
    {
      "epoch": 0.07076378740096916,
      "grad_norm": 1.3215057849884033,
      "learning_rate": 1.9916068131670302e-05,
      "loss": 1.5166,
      "step": 230
    },
    {
      "epoch": 0.07384047380970694,
      "grad_norm": 1.8699951171875,
      "learning_rate": 1.9902680687415704e-05,
      "loss": 1.5312,
      "step": 240
    },
    {
      "epoch": 0.07691716021844473,
      "grad_norm": 1.9110246896743774,
      "learning_rate": 1.9888308262251286e-05,
      "loss": 1.5371,
      "step": 250
    },
    {
      "epoch": 0.07999384662718252,
      "grad_norm": 1.5757243633270264,
      "learning_rate": 1.9872952285745958e-05,
      "loss": 1.4795,
      "step": 260
    },
    {
      "epoch": 0.08307053303592031,
      "grad_norm": 1.6495912075042725,
      "learning_rate": 1.985661428529863e-05,
      "loss": 1.5024,
      "step": 270
    },
    {
      "epoch": 0.0861472194446581,
      "grad_norm": 1.4374393224716187,
      "learning_rate": 1.98392958859863e-05,
      "loss": 1.4536,
      "step": 280
    },
    {
      "epoch": 0.08922390585339589,
      "grad_norm": 1.5605038404464722,
      "learning_rate": 1.982099881040239e-05,
      "loss": 1.5047,
      "step": 290
    },
    {
      "epoch": 0.09230059226213368,
      "grad_norm": 1.4167972803115845,
      "learning_rate": 1.9801724878485438e-05,
      "loss": 1.5137,
      "step": 300
    },
    {
      "epoch": 0.09537727867087147,
      "grad_norm": 1.7783607244491577,
      "learning_rate": 1.9781476007338058e-05,
      "loss": 1.5779,
      "step": 310
    },
    {
      "epoch": 0.09845396507960925,
      "grad_norm": 1.5667141675949097,
      "learning_rate": 1.9760254211036245e-05,
      "loss": 1.4542,
      "step": 320
    },
    {
      "epoch": 0.10153065148834706,
      "grad_norm": 2.0347747802734375,
      "learning_rate": 1.9738061600429062e-05,
      "loss": 1.5451,
      "step": 330
    },
    {
      "epoch": 0.10460733789708485,
      "grad_norm": 1.3531060218811035,
      "learning_rate": 1.9714900382928674e-05,
      "loss": 1.4235,
      "step": 340
    },
    {
      "epoch": 0.10768402430582263,
      "grad_norm": 1.514403223991394,
      "learning_rate": 1.969077286229078e-05,
      "loss": 1.458,
      "step": 350
    },
    {
      "epoch": 0.11076071071456042,
      "grad_norm": 1.7431132793426514,
      "learning_rate": 1.9665681438385475e-05,
      "loss": 1.4725,
      "step": 360
    },
    {
      "epoch": 0.11383739712329821,
      "grad_norm": 1.5374265909194946,
      "learning_rate": 1.9639628606958535e-05,
      "loss": 1.483,
      "step": 370
    },
    {
      "epoch": 0.116914083532036,
      "grad_norm": 1.464990258216858,
      "learning_rate": 1.961261695938319e-05,
      "loss": 1.5392,
      "step": 380
    },
    {
      "epoch": 0.11999076994077379,
      "grad_norm": 2.1966845989227295,
      "learning_rate": 1.9584649182402358e-05,
      "loss": 1.5239,
      "step": 390
    },
    {
      "epoch": 0.12306745634951158,
      "grad_norm": 1.5933136940002441,
      "learning_rate": 1.955572805786141e-05,
      "loss": 1.4837,
      "step": 400
    },
    {
      "epoch": 0.12614414275824937,
      "grad_norm": 1.3785812854766846,
      "learning_rate": 1.9525856462431463e-05,
      "loss": 1.4306,
      "step": 410
    },
    {
      "epoch": 0.12922082916698716,
      "grad_norm": 1.3829820156097412,
      "learning_rate": 1.9495037367323264e-05,
      "loss": 1.5642,
      "step": 420
    },
    {
      "epoch": 0.13229751557572494,
      "grad_norm": 2.3891384601593018,
      "learning_rate": 1.9463273837991643e-05,
      "loss": 1.6054,
      "step": 430
    },
    {
      "epoch": 0.13537420198446273,
      "grad_norm": 2.069030284881592,
      "learning_rate": 1.9430569033830606e-05,
      "loss": 1.4302,
      "step": 440
    },
    {
      "epoch": 0.13845088839320052,
      "grad_norm": 1.408713698387146,
      "learning_rate": 1.9396926207859085e-05,
      "loss": 1.4972,
      "step": 450
    },
    {
      "epoch": 0.1415275748019383,
      "grad_norm": 1.3923649787902832,
      "learning_rate": 1.9362348706397374e-05,
      "loss": 1.4098,
      "step": 460
    },
    {
      "epoch": 0.1446042612106761,
      "grad_norm": 2.2191271781921387,
      "learning_rate": 1.9326839968734278e-05,
      "loss": 1.4942,
      "step": 470
    },
    {
      "epoch": 0.1476809476194139,
      "grad_norm": 1.766045093536377,
      "learning_rate": 1.9290403526785025e-05,
      "loss": 1.5865,
      "step": 480
    },
    {
      "epoch": 0.15075763402815168,
      "grad_norm": 1.5753682851791382,
      "learning_rate": 1.9253043004739967e-05,
      "loss": 1.4174,
      "step": 490
    },
    {
      "epoch": 0.15383432043688947,
      "grad_norm": 1.4988203048706055,
      "learning_rate": 1.921476211870408e-05,
      "loss": 1.4477,
      "step": 500
    },
    {
      "epoch": 0.15691100684562725,
      "grad_norm": 1.9799903631210327,
      "learning_rate": 1.917556467632734e-05,
      "loss": 1.4577,
      "step": 510
    },
    {
      "epoch": 0.15998769325436504,
      "grad_norm": 1.6120448112487793,
      "learning_rate": 1.913545457642601e-05,
      "loss": 1.5234,
      "step": 520
    },
    {
      "epoch": 0.16306437966310283,
      "grad_norm": 2.075356960296631,
      "learning_rate": 1.9094435808594823e-05,
      "loss": 1.4883,
      "step": 530
    },
    {
      "epoch": 0.16614106607184062,
      "grad_norm": 1.6941807270050049,
      "learning_rate": 1.905251245281015e-05,
      "loss": 1.4736,
      "step": 540
    },
    {
      "epoch": 0.1692177524805784,
      "grad_norm": 1.8189822435379028,
      "learning_rate": 1.900968867902419e-05,
      "loss": 1.4903,
      "step": 550
    },
    {
      "epoch": 0.1722944388893162,
      "grad_norm": 1.786867380142212,
      "learning_rate": 1.896596874675021e-05,
      "loss": 1.4476,
      "step": 560
    },
    {
      "epoch": 0.175371125298054,
      "grad_norm": 1.4365131855010986,
      "learning_rate": 1.8921357004638837e-05,
      "loss": 1.5494,
      "step": 570
    },
    {
      "epoch": 0.17844781170679178,
      "grad_norm": 1.292417287826538,
      "learning_rate": 1.8875857890045544e-05,
      "loss": 1.5045,
      "step": 580
    },
    {
      "epoch": 0.18152449811552956,
      "grad_norm": 1.8869634866714478,
      "learning_rate": 1.8829475928589272e-05,
      "loss": 1.381,
      "step": 590
    },
    {
      "epoch": 0.18460118452426735,
      "grad_norm": 1.630769968032837,
      "learning_rate": 1.8782215733702286e-05,
      "loss": 1.4893,
      "step": 600
    },
    {
      "epoch": 0.18767787093300514,
      "grad_norm": 1.9891513586044312,
      "learning_rate": 1.87340820061713e-05,
      "loss": 1.4976,
      "step": 610
    },
    {
      "epoch": 0.19075455734174293,
      "grad_norm": 2.147393226623535,
      "learning_rate": 1.868507953366989e-05,
      "loss": 1.4708,
      "step": 620
    },
    {
      "epoch": 0.19383124375048072,
      "grad_norm": 1.6430387496948242,
      "learning_rate": 1.8635213190282312e-05,
      "loss": 1.4841,
      "step": 630
    },
    {
      "epoch": 0.1969079301592185,
      "grad_norm": 1.3435362577438354,
      "learning_rate": 1.8584487936018663e-05,
      "loss": 1.4889,
      "step": 640
    },
    {
      "epoch": 0.19998461656795632,
      "grad_norm": 1.8004708290100098,
      "learning_rate": 1.8532908816321557e-05,
      "loss": 1.5192,
      "step": 650
    },
    {
      "epoch": 0.2030613029766941,
      "grad_norm": 1.6184816360473633,
      "learning_rate": 1.848048096156426e-05,
      "loss": 1.4763,
      "step": 660
    },
    {
      "epoch": 0.2061379893854319,
      "grad_norm": 1.68843412399292,
      "learning_rate": 1.8427209586540392e-05,
      "loss": 1.4778,
      "step": 670
    },
    {
      "epoch": 0.2092146757941697,
      "grad_norm": 2.0412232875823975,
      "learning_rate": 1.8373099989945236e-05,
      "loss": 1.4832,
      "step": 680
    },
    {
      "epoch": 0.21229136220290748,
      "grad_norm": 1.530712604522705,
      "learning_rate": 1.8318157553848694e-05,
      "loss": 1.4502,
      "step": 690
    },
    {
      "epoch": 0.21536804861164527,
      "grad_norm": 2.020221710205078,
      "learning_rate": 1.826238774315995e-05,
      "loss": 1.4699,
      "step": 700
    },
    {
      "epoch": 0.21844473502038306,
      "grad_norm": 1.7086896896362305,
      "learning_rate": 1.8205796105083917e-05,
      "loss": 1.5198,
      "step": 710
    },
    {
      "epoch": 0.22152142142912085,
      "grad_norm": 1.5214827060699463,
      "learning_rate": 1.8148388268569453e-05,
      "loss": 1.4762,
      "step": 720
    },
    {
      "epoch": 0.22459810783785863,
      "grad_norm": 1.9894205331802368,
      "learning_rate": 1.8090169943749477e-05,
      "loss": 1.5765,
      "step": 730
    },
    {
      "epoch": 0.22767479424659642,
      "grad_norm": 1.7415210008621216,
      "learning_rate": 1.803114692137302e-05,
      "loss": 1.361,
      "step": 740
    },
    {
      "epoch": 0.2307514806553342,
      "grad_norm": 1.6149667501449585,
      "learning_rate": 1.7971325072229227e-05,
      "loss": 1.5151,
      "step": 750
    },
    {
      "epoch": 0.233828167064072,
      "grad_norm": 1.853452444076538,
      "learning_rate": 1.7910710346563417e-05,
      "loss": 1.5116,
      "step": 760
    },
    {
      "epoch": 0.2369048534728098,
      "grad_norm": 1.6080070734024048,
      "learning_rate": 1.7849308773485226e-05,
      "loss": 1.4276,
      "step": 770
    },
    {
      "epoch": 0.23998153988154758,
      "grad_norm": 1.739551067352295,
      "learning_rate": 1.778712646036894e-05,
      "loss": 1.4618,
      "step": 780
    },
    {
      "epoch": 0.24305822629028537,
      "grad_norm": 1.4252994060516357,
      "learning_rate": 1.7724169592245996e-05,
      "loss": 1.5302,
      "step": 790
    },
    {
      "epoch": 0.24613491269902316,
      "grad_norm": 1.8118815422058105,
      "learning_rate": 1.766044443118978e-05,
      "loss": 1.5252,
      "step": 800
    },
    {
      "epoch": 0.24921159910776094,
      "grad_norm": 2.245774030685425,
      "learning_rate": 1.7595957315692782e-05,
      "loss": 1.4798,
      "step": 810
    },
    {
      "epoch": 0.25228828551649873,
      "grad_norm": 1.7938867807388306,
      "learning_rate": 1.7530714660036112e-05,
      "loss": 1.4149,
      "step": 820
    },
    {
      "epoch": 0.2553649719252365,
      "grad_norm": 2.096611976623535,
      "learning_rate": 1.7464722953651504e-05,
      "loss": 1.4659,
      "step": 830
    },
    {
      "epoch": 0.2584416583339743,
      "grad_norm": 1.5865753889083862,
      "learning_rate": 1.7397988760475842e-05,
      "loss": 1.4449,
      "step": 840
    },
    {
      "epoch": 0.26151834474271207,
      "grad_norm": 1.672377109527588,
      "learning_rate": 1.7330518718298263e-05,
      "loss": 1.4637,
      "step": 850
    },
    {
      "epoch": 0.2645950311514499,
      "grad_norm": 1.519490122795105,
      "learning_rate": 1.726231953809993e-05,
      "loss": 1.4958,
      "step": 860
    },
    {
      "epoch": 0.2676717175601877,
      "grad_norm": 2.372631072998047,
      "learning_rate": 1.7193398003386514e-05,
      "loss": 1.4378,
      "step": 870
    },
    {
      "epoch": 0.27074840396892547,
      "grad_norm": 1.8442955017089844,
      "learning_rate": 1.7123760969513448e-05,
      "loss": 1.4275,
      "step": 880
    },
    {
      "epoch": 0.2738250903776633,
      "grad_norm": 1.9961371421813965,
      "learning_rate": 1.705341536300409e-05,
      "loss": 1.5167,
      "step": 890
    },
    {
      "epoch": 0.27690177678640104,
      "grad_norm": 2.4113881587982178,
      "learning_rate": 1.698236818086073e-05,
      "loss": 1.4857,
      "step": 900
    },
    {
      "epoch": 0.27997846319513886,
      "grad_norm": 2.050039052963257,
      "learning_rate": 1.691062648986865e-05,
      "loss": 1.4564,
      "step": 910
    },
    {
      "epoch": 0.2830551496038766,
      "grad_norm": 1.4196875095367432,
      "learning_rate": 1.68381974258932e-05,
      "loss": 1.4904,
      "step": 920
    },
    {
      "epoch": 0.28613183601261444,
      "grad_norm": 2.326888084411621,
      "learning_rate": 1.6765088193170055e-05,
      "loss": 1.444,
      "step": 930
    },
    {
      "epoch": 0.2892085224213522,
      "grad_norm": 1.5967153310775757,
      "learning_rate": 1.6691306063588583e-05,
      "loss": 1.4269,
      "step": 940
    },
    {
      "epoch": 0.29228520883009,
      "grad_norm": 1.7992665767669678,
      "learning_rate": 1.6616858375968596e-05,
      "loss": 1.4366,
      "step": 950
    },
    {
      "epoch": 0.2953618952388278,
      "grad_norm": 1.726563811302185,
      "learning_rate": 1.6541752535330345e-05,
      "loss": 1.47,
      "step": 960
    },
    {
      "epoch": 0.2984385816475656,
      "grad_norm": 1.3604480028152466,
      "learning_rate": 1.6465996012157996e-05,
      "loss": 1.5062,
      "step": 970
    },
    {
      "epoch": 0.30151526805630335,
      "grad_norm": 1.4974393844604492,
      "learning_rate": 1.638959634165656e-05,
      "loss": 1.4403,
      "step": 980
    },
    {
      "epoch": 0.30459195446504117,
      "grad_norm": 1.9502745866775513,
      "learning_rate": 1.631256112300239e-05,
      "loss": 1.4698,
      "step": 990
    },
    {
      "epoch": 0.30766864087377893,
      "grad_norm": 1.8093303442001343,
      "learning_rate": 1.6234898018587336e-05,
      "loss": 1.4744,
      "step": 1000
    },
    {
      "epoch": 0.31074532728251675,
      "grad_norm": 1.840048909187317,
      "learning_rate": 1.6156614753256587e-05,
      "loss": 1.4645,
      "step": 1010
    },
    {
      "epoch": 0.3138220136912545,
      "grad_norm": 1.6941078901290894,
      "learning_rate": 1.6077719113540303e-05,
      "loss": 1.4401,
      "step": 1020
    },
    {
      "epoch": 0.3168987000999923,
      "grad_norm": 2.0276987552642822,
      "learning_rate": 1.599821894687914e-05,
      "loss": 1.4414,
      "step": 1030
    },
    {
      "epoch": 0.3199753865087301,
      "grad_norm": 2.038058042526245,
      "learning_rate": 1.591812216084368e-05,
      "loss": 1.4278,
      "step": 1040
    },
    {
      "epoch": 0.3230520729174679,
      "grad_norm": 1.7369332313537598,
      "learning_rate": 1.5837436722347902e-05,
      "loss": 1.4584,
      "step": 1050
    },
    {
      "epoch": 0.32612875932620566,
      "grad_norm": 1.6693401336669922,
      "learning_rate": 1.575617065685674e-05,
      "loss": 1.4068,
      "step": 1060
    },
    {
      "epoch": 0.3292054457349435,
      "grad_norm": 2.3533058166503906,
      "learning_rate": 1.567433204758782e-05,
      "loss": 1.4848,
      "step": 1070
    },
    {
      "epoch": 0.33228213214368124,
      "grad_norm": 1.5567822456359863,
      "learning_rate": 1.5591929034707468e-05,
      "loss": 1.4315,
      "step": 1080
    },
    {
      "epoch": 0.33535881855241906,
      "grad_norm": 1.5436983108520508,
      "learning_rate": 1.5508969814521026e-05,
      "loss": 1.4785,
      "step": 1090
    },
    {
      "epoch": 0.3384355049611568,
      "grad_norm": 1.8367791175842285,
      "learning_rate": 1.5425462638657597e-05,
      "loss": 1.482,
      "step": 1100
    },
    {
      "epoch": 0.34151219136989464,
      "grad_norm": 1.6836044788360596,
      "learning_rate": 1.534141581324929e-05,
      "loss": 1.5116,
      "step": 1110
    },
    {
      "epoch": 0.3445888777786324,
      "grad_norm": 1.8742570877075195,
      "learning_rate": 1.5256837698105047e-05,
      "loss": 1.4217,
      "step": 1120
    },
    {
      "epoch": 0.3476655641873702,
      "grad_norm": 2.4306979179382324,
      "learning_rate": 1.5171736705879127e-05,
      "loss": 1.4327,
      "step": 1130
    },
    {
      "epoch": 0.350742250596108,
      "grad_norm": 1.5646089315414429,
      "learning_rate": 1.5086121301234318e-05,
      "loss": 1.4512,
      "step": 1140
    },
    {
      "epoch": 0.3538189370048458,
      "grad_norm": 2.1491034030914307,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 1.4754,
      "step": 1150
    },
    {
      "epoch": 0.35689562341358355,
      "grad_norm": 1.523507833480835,
      "learning_rate": 1.4913381368325115e-05,
      "loss": 1.422,
      "step": 1160
    },
    {
      "epoch": 0.35997230982232137,
      "grad_norm": 1.8149958848953247,
      "learning_rate": 1.482627402182611e-05,
      "loss": 1.419,
      "step": 1170
    },
    {
      "epoch": 0.36304899623105913,
      "grad_norm": 1.8757041692733765,
      "learning_rate": 1.4738686624729987e-05,
      "loss": 1.4094,
      "step": 1180
    },
    {
      "epoch": 0.36612568263979695,
      "grad_norm": 1.6085072755813599,
      "learning_rate": 1.4650627889012507e-05,
      "loss": 1.4631,
      "step": 1190
    },
    {
      "epoch": 0.3692023690485347,
      "grad_norm": 1.7146810293197632,
      "learning_rate": 1.4562106573531632e-05,
      "loss": 1.4345,
      "step": 1200
    },
    {
      "epoch": 0.3722790554572725,
      "grad_norm": 1.8090571165084839,
      "learning_rate": 1.4473131483156326e-05,
      "loss": 1.3737,
      "step": 1210
    },
    {
      "epoch": 0.3753557418660103,
      "grad_norm": 1.8342642784118652,
      "learning_rate": 1.4383711467890776e-05,
      "loss": 1.4935,
      "step": 1220
    },
    {
      "epoch": 0.3784324282747481,
      "grad_norm": 1.5342918634414673,
      "learning_rate": 1.4293855421994094e-05,
      "loss": 1.4338,
      "step": 1230
    },
    {
      "epoch": 0.38150911468348586,
      "grad_norm": 1.8408302068710327,
      "learning_rate": 1.4203572283095657e-05,
      "loss": 1.4467,
      "step": 1240
    },
    {
      "epoch": 0.3845858010922237,
      "grad_norm": 1.7343922853469849,
      "learning_rate": 1.4112871031306118e-05,
      "loss": 1.4306,
      "step": 1250
    },
    {
      "epoch": 0.38766248750096144,
      "grad_norm": 2.49643611907959,
      "learning_rate": 1.4021760688324175e-05,
      "loss": 1.4506,
      "step": 1260
    },
    {
      "epoch": 0.39073917390969926,
      "grad_norm": 1.8977738618850708,
      "learning_rate": 1.3930250316539237e-05,
      "loss": 1.4282,
      "step": 1270
    },
    {
      "epoch": 0.393815860318437,
      "grad_norm": 2.1465680599212646,
      "learning_rate": 1.3838349018130007e-05,
      "loss": 1.4426,
      "step": 1280
    },
    {
      "epoch": 0.39689254672717483,
      "grad_norm": 2.0027425289154053,
      "learning_rate": 1.3746065934159123e-05,
      "loss": 1.4383,
      "step": 1290
    },
    {
      "epoch": 0.39996923313591265,
      "grad_norm": 2.1128287315368652,
      "learning_rate": 1.3653410243663953e-05,
      "loss": 1.4968,
      "step": 1300
    },
    {
      "epoch": 0.4030459195446504,
      "grad_norm": 2.0714821815490723,
      "learning_rate": 1.356039116274357e-05,
      "loss": 1.4659,
      "step": 1310
    },
    {
      "epoch": 0.4061226059533882,
      "grad_norm": 1.9093948602676392,
      "learning_rate": 1.3467017943642074e-05,
      "loss": 1.5237,
      "step": 1320
    },
    {
      "epoch": 0.409199292362126,
      "grad_norm": 2.1548590660095215,
      "learning_rate": 1.3373299873828303e-05,
      "loss": 1.5549,
      "step": 1330
    },
    {
      "epoch": 0.4122759787708638,
      "grad_norm": 1.676721453666687,
      "learning_rate": 1.3279246275072046e-05,
      "loss": 1.4753,
      "step": 1340
    },
    {
      "epoch": 0.41535266517960157,
      "grad_norm": 1.8964132070541382,
      "learning_rate": 1.3184866502516846e-05,
      "loss": 1.4278,
      "step": 1350
    },
    {
      "epoch": 0.4184293515883394,
      "grad_norm": 1.8993810415267944,
      "learning_rate": 1.3090169943749475e-05,
      "loss": 1.3906,
      "step": 1360
    },
    {
      "epoch": 0.42150603799707714,
      "grad_norm": 1.782896637916565,
      "learning_rate": 1.2995166017866194e-05,
      "loss": 1.4746,
      "step": 1370
    },
    {
      "epoch": 0.42458272440581496,
      "grad_norm": 1.598700761795044,
      "learning_rate": 1.2899864174535863e-05,
      "loss": 1.3676,
      "step": 1380
    },
    {
      "epoch": 0.4276594108145527,
      "grad_norm": 2.2615458965301514,
      "learning_rate": 1.2804273893060028e-05,
      "loss": 1.4233,
      "step": 1390
    },
    {
      "epoch": 0.43073609722329054,
      "grad_norm": 1.5437119007110596,
      "learning_rate": 1.2708404681430054e-05,
      "loss": 1.4005,
      "step": 1400
    },
    {
      "epoch": 0.4338127836320283,
      "grad_norm": 1.5093774795532227,
      "learning_rate": 1.2612266075381385e-05,
      "loss": 1.4566,
      "step": 1410
    },
    {
      "epoch": 0.4368894700407661,
      "grad_norm": 1.9269882440567017,
      "learning_rate": 1.2515867637445088e-05,
      "loss": 1.4718,
      "step": 1420
    },
    {
      "epoch": 0.4399661564495039,
      "grad_norm": 1.7558374404907227,
      "learning_rate": 1.2419218955996677e-05,
      "loss": 1.5307,
      "step": 1430
    },
    {
      "epoch": 0.4430428428582417,
      "grad_norm": 1.340081810951233,
      "learning_rate": 1.2322329644302426e-05,
      "loss": 1.4502,
      "step": 1440
    },
    {
      "epoch": 0.44611952926697945,
      "grad_norm": 1.5883005857467651,
      "learning_rate": 1.2225209339563144e-05,
      "loss": 1.5104,
      "step": 1450
    },
    {
      "epoch": 0.44919621567571727,
      "grad_norm": 1.7270656824111938,
      "learning_rate": 1.2127867701955622e-05,
      "loss": 1.4792,
      "step": 1460
    },
    {
      "epoch": 0.45227290208445503,
      "grad_norm": 2.263986349105835,
      "learning_rate": 1.2030314413671763e-05,
      "loss": 1.4588,
      "step": 1470
    },
    {
      "epoch": 0.45534958849319285,
      "grad_norm": 1.743054986000061,
      "learning_rate": 1.1932559177955533e-05,
      "loss": 1.4579,
      "step": 1480
    },
    {
      "epoch": 0.4584262749019306,
      "grad_norm": 1.7528936862945557,
      "learning_rate": 1.1834611718137825e-05,
      "loss": 1.4721,
      "step": 1490
    },
    {
      "epoch": 0.4615029613106684,
      "grad_norm": 2.0094234943389893,
      "learning_rate": 1.1736481776669307e-05,
      "loss": 1.4879,
      "step": 1500
    },
    {
      "epoch": 0.4645796477194062,
      "grad_norm": 1.9198551177978516,
      "learning_rate": 1.1638179114151378e-05,
      "loss": 1.4696,
      "step": 1510
    },
    {
      "epoch": 0.467656334128144,
      "grad_norm": 2.1392698287963867,
      "learning_rate": 1.1539713508365336e-05,
      "loss": 1.4577,
      "step": 1520
    },
    {
      "epoch": 0.47073302053688176,
      "grad_norm": 1.744079828262329,
      "learning_rate": 1.1441094753299802e-05,
      "loss": 1.4351,
      "step": 1530
    },
    {
      "epoch": 0.4738097069456196,
      "grad_norm": 1.8205838203430176,
      "learning_rate": 1.1342332658176556e-05,
      "loss": 1.4642,
      "step": 1540
    },
    {
      "epoch": 0.47688639335435734,
      "grad_norm": 1.3391807079315186,
      "learning_rate": 1.1243437046474854e-05,
      "loss": 1.4181,
      "step": 1550
    },
    {
      "epoch": 0.47996307976309516,
      "grad_norm": 2.398071050643921,
      "learning_rate": 1.114441775495432e-05,
      "loss": 1.4364,
      "step": 1560
    },
    {
      "epoch": 0.4830397661718329,
      "grad_norm": 1.618969440460205,
      "learning_rate": 1.1045284632676535e-05,
      "loss": 1.4052,
      "step": 1570
    },
    {
      "epoch": 0.48611645258057073,
      "grad_norm": 2.0174899101257324,
      "learning_rate": 1.0946047540025373e-05,
      "loss": 1.4344,
      "step": 1580
    },
    {
      "epoch": 0.4891931389893085,
      "grad_norm": 1.8798362016677856,
      "learning_rate": 1.0846716347726233e-05,
      "loss": 1.4496,
      "step": 1590
    },
    {
      "epoch": 0.4922698253980463,
      "grad_norm": 2.014975070953369,
      "learning_rate": 1.0747300935864245e-05,
      "loss": 1.4825,
      "step": 1600
    },
    {
      "epoch": 0.4953465118067841,
      "grad_norm": 1.6376944780349731,
      "learning_rate": 1.0647811192901518e-05,
      "loss": 1.4794,
      "step": 1610
    },
    {
      "epoch": 0.4984231982155219,
      "grad_norm": 1.9758388996124268,
      "learning_rate": 1.0548257014693602e-05,
      "loss": 1.5046,
      "step": 1620
    },
    {
      "epoch": 0.5014998846242597,
      "grad_norm": 2.9328861236572266,
      "learning_rate": 1.044864830350515e-05,
      "loss": 1.4117,
      "step": 1630
    },
    {
      "epoch": 0.5045765710329975,
      "grad_norm": 2.23486065864563,
      "learning_rate": 1.0348994967025012e-05,
      "loss": 1.4799,
      "step": 1640
    },
    {
      "epoch": 0.5076532574417353,
      "grad_norm": 1.9865061044692993,
      "learning_rate": 1.0249306917380731e-05,
      "loss": 1.4094,
      "step": 1650
    },
    {
      "epoch": 0.510729943850473,
      "grad_norm": 1.6531007289886475,
      "learning_rate": 1.0149594070152638e-05,
      "loss": 1.4649,
      "step": 1660
    },
    {
      "epoch": 0.5138066302592108,
      "grad_norm": 2.378404140472412,
      "learning_rate": 1.0049866343387582e-05,
      "loss": 1.4389,
      "step": 1670
    },
    {
      "epoch": 0.5168833166679486,
      "grad_norm": 2.1131792068481445,
      "learning_rate": 9.950133656612421e-06,
      "loss": 1.4813,
      "step": 1680
    },
    {
      "epoch": 0.5199600030766864,
      "grad_norm": 1.9383997917175293,
      "learning_rate": 9.850405929847367e-06,
      "loss": 1.4696,
      "step": 1690
    },
    {
      "epoch": 0.5230366894854241,
      "grad_norm": 2.275620698928833,
      "learning_rate": 9.750693082619274e-06,
      "loss": 1.4131,
      "step": 1700
    },
    {
      "epoch": 0.526113375894162,
      "grad_norm": 1.7879891395568848,
      "learning_rate": 9.651005032974994e-06,
      "loss": 1.4644,
      "step": 1710
    },
    {
      "epoch": 0.5291900623028998,
      "grad_norm": 1.9138652086257935,
      "learning_rate": 9.551351696494854e-06,
      "loss": 1.4645,
      "step": 1720
    },
    {
      "epoch": 0.5322667487116376,
      "grad_norm": 2.0169148445129395,
      "learning_rate": 9.4517429853064e-06,
      "loss": 1.458,
      "step": 1730
    },
    {
      "epoch": 0.5353434351203754,
      "grad_norm": 2.1194841861724854,
      "learning_rate": 9.352188807098482e-06,
      "loss": 1.422,
      "step": 1740
    },
    {
      "epoch": 0.5384201215291131,
      "grad_norm": 1.8011881113052368,
      "learning_rate": 9.252699064135759e-06,
      "loss": 1.3825,
      "step": 1750
    },
    {
      "epoch": 0.5414968079378509,
      "grad_norm": 1.7854007482528687,
      "learning_rate": 9.153283652273768e-06,
      "loss": 1.4305,
      "step": 1760
    },
    {
      "epoch": 0.5445734943465887,
      "grad_norm": 1.9859312772750854,
      "learning_rate": 9.05395245997463e-06,
      "loss": 1.4726,
      "step": 1770
    },
    {
      "epoch": 0.5476501807553266,
      "grad_norm": 1.812432050704956,
      "learning_rate": 8.954715367323468e-06,
      "loss": 1.4799,
      "step": 1780
    },
    {
      "epoch": 0.5507268671640643,
      "grad_norm": 2.6492085456848145,
      "learning_rate": 8.855582245045682e-06,
      "loss": 1.4783,
      "step": 1790
    },
    {
      "epoch": 0.5538035535728021,
      "grad_norm": 1.581897258758545,
      "learning_rate": 8.756562953525151e-06,
      "loss": 1.4417,
      "step": 1800
    },
    {
      "epoch": 0.5568802399815399,
      "grad_norm": 1.7481697797775269,
      "learning_rate": 8.657667341823449e-06,
      "loss": 1.4611,
      "step": 1810
    },
    {
      "epoch": 0.5599569263902777,
      "grad_norm": 1.9234176874160767,
      "learning_rate": 8.558905246700202e-06,
      "loss": 1.4967,
      "step": 1820
    },
    {
      "epoch": 0.5630336127990154,
      "grad_norm": 1.7232259511947632,
      "learning_rate": 8.460286491634664e-06,
      "loss": 1.447,
      "step": 1830
    },
    {
      "epoch": 0.5661102992077532,
      "grad_norm": 1.7879465818405151,
      "learning_rate": 8.361820885848623e-06,
      "loss": 1.4763,
      "step": 1840
    },
    {
      "epoch": 0.5691869856164911,
      "grad_norm": 2.329395055770874,
      "learning_rate": 8.263518223330698e-06,
      "loss": 1.4363,
      "step": 1850
    },
    {
      "epoch": 0.5722636720252289,
      "grad_norm": 1.9560860395431519,
      "learning_rate": 8.165388281862177e-06,
      "loss": 1.4519,
      "step": 1860
    },
    {
      "epoch": 0.5753403584339666,
      "grad_norm": 2.096571922302246,
      "learning_rate": 8.06744082204447e-06,
      "loss": 1.4193,
      "step": 1870
    },
    {
      "epoch": 0.5784170448427044,
      "grad_norm": 2.374741792678833,
      "learning_rate": 7.96968558632824e-06,
      "loss": 1.4976,
      "step": 1880
    },
    {
      "epoch": 0.5814937312514422,
      "grad_norm": 2.078611373901367,
      "learning_rate": 7.872132298044382e-06,
      "loss": 1.4344,
      "step": 1890
    },
    {
      "epoch": 0.58457041766018,
      "grad_norm": 1.755004644393921,
      "learning_rate": 7.774790660436857e-06,
      "loss": 1.4641,
      "step": 1900
    },
    {
      "epoch": 0.5876471040689177,
      "grad_norm": 1.9605672359466553,
      "learning_rate": 7.677670355697577e-06,
      "loss": 1.4591,
      "step": 1910
    },
    {
      "epoch": 0.5907237904776556,
      "grad_norm": 1.9534884691238403,
      "learning_rate": 7.580781044003324e-06,
      "loss": 1.421,
      "step": 1920
    },
    {
      "epoch": 0.5938004768863934,
      "grad_norm": 1.8837711811065674,
      "learning_rate": 7.484132362554915e-06,
      "loss": 1.4846,
      "step": 1930
    },
    {
      "epoch": 0.5968771632951312,
      "grad_norm": 1.9229437112808228,
      "learning_rate": 7.387733924618617e-06,
      "loss": 1.5011,
      "step": 1940
    },
    {
      "epoch": 0.5999538497038689,
      "grad_norm": 2.1550941467285156,
      "learning_rate": 7.291595318569951e-06,
      "loss": 1.4567,
      "step": 1950
    },
    {
      "epoch": 0.6030305361126067,
      "grad_norm": 2.202733039855957,
      "learning_rate": 7.1957261069399745e-06,
      "loss": 1.4063,
      "step": 1960
    },
    {
      "epoch": 0.6061072225213445,
      "grad_norm": 2.270540237426758,
      "learning_rate": 7.100135825464138e-06,
      "loss": 1.398,
      "step": 1970
    },
    {
      "epoch": 0.6091839089300823,
      "grad_norm": 2.7377068996429443,
      "learning_rate": 7.004833982133808e-06,
      "loss": 1.491,
      "step": 1980
    },
    {
      "epoch": 0.61226059533882,
      "grad_norm": 1.7985970973968506,
      "learning_rate": 6.909830056250527e-06,
      "loss": 1.5134,
      "step": 1990
    },
    {
      "epoch": 0.6153372817475579,
      "grad_norm": 1.5906840562820435,
      "learning_rate": 6.815133497483157e-06,
      "loss": 1.4596,
      "step": 2000
    },
    {
      "epoch": 0.6184139681562957,
      "grad_norm": 1.9529569149017334,
      "learning_rate": 6.720753724927957e-06,
      "loss": 1.4094,
      "step": 2010
    },
    {
      "epoch": 0.6214906545650335,
      "grad_norm": 1.908858060836792,
      "learning_rate": 6.6267001261717015e-06,
      "loss": 1.4473,
      "step": 2020
    },
    {
      "epoch": 0.6245673409737712,
      "grad_norm": 1.9376755952835083,
      "learning_rate": 6.532982056357928e-06,
      "loss": 1.4359,
      "step": 2030
    },
    {
      "epoch": 0.627644027382509,
      "grad_norm": 2.137190580368042,
      "learning_rate": 6.439608837256432e-06,
      "loss": 1.4949,
      "step": 2040
    },
    {
      "epoch": 0.6307207137912468,
      "grad_norm": 1.960960865020752,
      "learning_rate": 6.34658975633605e-06,
      "loss": 1.4068,
      "step": 2050
    },
    {
      "epoch": 0.6337974001999847,
      "grad_norm": 1.9425417184829712,
      "learning_rate": 6.25393406584088e-06,
      "loss": 1.4513,
      "step": 2060
    },
    {
      "epoch": 0.6368740866087224,
      "grad_norm": 1.9605761766433716,
      "learning_rate": 6.1616509818699975e-06,
      "loss": 1.36,
      "step": 2070
    },
    {
      "epoch": 0.6399507730174602,
      "grad_norm": 2.24804425239563,
      "learning_rate": 6.069749683460765e-06,
      "loss": 1.4551,
      "step": 2080
    },
    {
      "epoch": 0.643027459426198,
      "grad_norm": 2.2846648693084717,
      "learning_rate": 5.978239311675826e-06,
      "loss": 1.4264,
      "step": 2090
    },
    {
      "epoch": 0.6461041458349358,
      "grad_norm": 2.242269277572632,
      "learning_rate": 5.887128968693887e-06,
      "loss": 1.4478,
      "step": 2100
    },
    {
      "epoch": 0.6491808322436735,
      "grad_norm": 1.7147783041000366,
      "learning_rate": 5.796427716904347e-06,
      "loss": 1.4357,
      "step": 2110
    },
    {
      "epoch": 0.6522575186524113,
      "grad_norm": 1.9232838153839111,
      "learning_rate": 5.706144578005908e-06,
      "loss": 1.5152,
      "step": 2120
    },
    {
      "epoch": 0.6553342050611491,
      "grad_norm": 2.0441126823425293,
      "learning_rate": 5.616288532109225e-06,
      "loss": 1.4853,
      "step": 2130
    },
    {
      "epoch": 0.658410891469887,
      "grad_norm": 1.482347011566162,
      "learning_rate": 5.526868516843673e-06,
      "loss": 1.4757,
      "step": 2140
    },
    {
      "epoch": 0.6614875778786247,
      "grad_norm": 1.739771842956543,
      "learning_rate": 5.43789342646837e-06,
      "loss": 1.4342,
      "step": 2150
    },
    {
      "epoch": 0.6645642642873625,
      "grad_norm": 1.7715109586715698,
      "learning_rate": 5.349372110987496e-06,
      "loss": 1.4293,
      "step": 2160
    },
    {
      "epoch": 0.6676409506961003,
      "grad_norm": 1.9624998569488525,
      "learning_rate": 5.2613133752700145e-06,
      "loss": 1.4846,
      "step": 2170
    },
    {
      "epoch": 0.6707176371048381,
      "grad_norm": 2.120252847671509,
      "learning_rate": 5.173725978173893e-06,
      "loss": 1.4491,
      "step": 2180
    },
    {
      "epoch": 0.6737943235135759,
      "grad_norm": 1.902995228767395,
      "learning_rate": 5.086618631674888e-06,
      "loss": 1.47,
      "step": 2190
    },
    {
      "epoch": 0.6768710099223136,
      "grad_norm": 1.8539515733718872,
      "learning_rate": 5.000000000000003e-06,
      "loss": 1.3461,
      "step": 2200
    },
    {
      "epoch": 0.6799476963310515,
      "grad_norm": 2.363842487335205,
      "learning_rate": 4.9138786987656865e-06,
      "loss": 1.397,
      "step": 2210
    },
    {
      "epoch": 0.6830243827397893,
      "grad_norm": 2.401677131652832,
      "learning_rate": 4.8282632941208725e-06,
      "loss": 1.4529,
      "step": 2220
    },
    {
      "epoch": 0.6861010691485271,
      "grad_norm": 2.0376367568969727,
      "learning_rate": 4.743162301894952e-06,
      "loss": 1.4406,
      "step": 2230
    },
    {
      "epoch": 0.6891777555572648,
      "grad_norm": 2.2593679428100586,
      "learning_rate": 4.658584186750713e-06,
      "loss": 1.4306,
      "step": 2240
    },
    {
      "epoch": 0.6922544419660026,
      "grad_norm": 1.9086968898773193,
      "learning_rate": 4.574537361342406e-06,
      "loss": 1.4359,
      "step": 2250
    },
    {
      "epoch": 0.6953311283747404,
      "grad_norm": 1.8533012866973877,
      "learning_rate": 4.491030185478976e-06,
      "loss": 1.4462,
      "step": 2260
    },
    {
      "epoch": 0.6984078147834782,
      "grad_norm": 1.7329587936401367,
      "learning_rate": 4.408070965292534e-06,
      "loss": 1.4689,
      "step": 2270
    },
    {
      "epoch": 0.701484501192216,
      "grad_norm": 1.9316153526306152,
      "learning_rate": 4.325667952412184e-06,
      "loss": 1.4385,
      "step": 2280
    },
    {
      "epoch": 0.7045611876009538,
      "grad_norm": 1.6582610607147217,
      "learning_rate": 4.2438293431432665e-06,
      "loss": 1.4052,
      "step": 2290
    },
    {
      "epoch": 0.7076378740096916,
      "grad_norm": 2.1650946140289307,
      "learning_rate": 4.162563277652104e-06,
      "loss": 1.3848,
      "step": 2300
    },
    {
      "epoch": 0.7107145604184294,
      "grad_norm": 1.9530688524246216,
      "learning_rate": 4.0818778391563255e-06,
      "loss": 1.3961,
      "step": 2310
    },
    {
      "epoch": 0.7137912468271671,
      "grad_norm": 1.5675512552261353,
      "learning_rate": 4.0017810531208624e-06,
      "loss": 1.4161,
      "step": 2320
    },
    {
      "epoch": 0.7168679332359049,
      "grad_norm": 1.7719640731811523,
      "learning_rate": 3.922280886459701e-06,
      "loss": 1.3724,
      "step": 2330
    },
    {
      "epoch": 0.7199446196446427,
      "grad_norm": 2.0010602474212646,
      "learning_rate": 3.8433852467434175e-06,
      "loss": 1.4486,
      "step": 2340
    },
    {
      "epoch": 0.7230213060533806,
      "grad_norm": 1.8506277799606323,
      "learning_rate": 3.7651019814126656e-06,
      "loss": 1.4167,
      "step": 2350
    },
    {
      "epoch": 0.7260979924621183,
      "grad_norm": 1.6299618482589722,
      "learning_rate": 3.687438876997611e-06,
      "loss": 1.5039,
      "step": 2360
    },
    {
      "epoch": 0.7291746788708561,
      "grad_norm": 2.0194249153137207,
      "learning_rate": 3.610403658343442e-06,
      "loss": 1.4243,
      "step": 2370
    },
    {
      "epoch": 0.7322513652795939,
      "grad_norm": 1.94874107837677,
      "learning_rate": 3.534003987842005e-06,
      "loss": 1.4077,
      "step": 2380
    },
    {
      "epoch": 0.7353280516883317,
      "grad_norm": 1.313286542892456,
      "learning_rate": 3.4582474646696575e-06,
      "loss": 1.388,
      "step": 2390
    },
    {
      "epoch": 0.7384047380970694,
      "grad_norm": 2.3349835872650146,
      "learning_rate": 3.3831416240314085e-06,
      "loss": 1.4512,
      "step": 2400
    },
    {
      "epoch": 0.7414814245058072,
      "grad_norm": 2.211681842803955,
      "learning_rate": 3.308693936411421e-06,
      "loss": 1.4462,
      "step": 2410
    },
    {
      "epoch": 0.744558110914545,
      "grad_norm": 2.035101890563965,
      "learning_rate": 3.234911806829948e-06,
      "loss": 1.3749,
      "step": 2420
    },
    {
      "epoch": 0.7476347973232829,
      "grad_norm": 1.6377754211425781,
      "learning_rate": 3.161802574106799e-06,
      "loss": 1.4562,
      "step": 2430
    },
    {
      "epoch": 0.7507114837320206,
      "grad_norm": 1.9539073705673218,
      "learning_rate": 3.089373510131354e-06,
      "loss": 1.4692,
      "step": 2440
    },
    {
      "epoch": 0.7537881701407584,
      "grad_norm": 1.8483625650405884,
      "learning_rate": 3.017631819139273e-06,
      "loss": 1.4168,
      "step": 2450
    },
    {
      "epoch": 0.7568648565494962,
      "grad_norm": 1.952438473701477,
      "learning_rate": 2.9465846369959126e-06,
      "loss": 1.4389,
      "step": 2460
    },
    {
      "epoch": 0.759941542958234,
      "grad_norm": 1.7072134017944336,
      "learning_rate": 2.876239030486554e-06,
      "loss": 1.4814,
      "step": 2470
    },
    {
      "epoch": 0.7630182293669717,
      "grad_norm": 2.093989372253418,
      "learning_rate": 2.8066019966134907e-06,
      "loss": 1.4163,
      "step": 2480
    },
    {
      "epoch": 0.7660949157757095,
      "grad_norm": 1.721861481666565,
      "learning_rate": 2.7376804619000706e-06,
      "loss": 1.4356,
      "step": 2490
    },
    {
      "epoch": 0.7691716021844474,
      "grad_norm": 1.8126308917999268,
      "learning_rate": 2.6694812817017403e-06,
      "loss": 1.4316,
      "step": 2500
    },
    {
      "epoch": 0.7722482885931852,
      "grad_norm": 1.7211109399795532,
      "learning_rate": 2.6020112395241635e-06,
      "loss": 1.4472,
      "step": 2510
    },
    {
      "epoch": 0.7753249750019229,
      "grad_norm": 2.4344894886016846,
      "learning_rate": 2.5352770463484986e-06,
      "loss": 1.4392,
      "step": 2520
    },
    {
      "epoch": 0.7784016614106607,
      "grad_norm": 1.6941437721252441,
      "learning_rate": 2.469285339963892e-06,
      "loss": 1.3928,
      "step": 2530
    },
    {
      "epoch": 0.7814783478193985,
      "grad_norm": 2.273315191268921,
      "learning_rate": 2.4040426843072206e-06,
      "loss": 1.463,
      "step": 2540
    },
    {
      "epoch": 0.7845550342281363,
      "grad_norm": 1.9633455276489258,
      "learning_rate": 2.339555568810221e-06,
      "loss": 1.4029,
      "step": 2550
    },
    {
      "epoch": 0.787631720636874,
      "grad_norm": 2.2397592067718506,
      "learning_rate": 2.2758304077540073e-06,
      "loss": 1.4214,
      "step": 2560
    },
    {
      "epoch": 0.7907084070456118,
      "grad_norm": 1.9349217414855957,
      "learning_rate": 2.212873539631062e-06,
      "loss": 1.4262,
      "step": 2570
    },
    {
      "epoch": 0.7937850934543497,
      "grad_norm": 1.9309085607528687,
      "learning_rate": 2.1506912265147772e-06,
      "loss": 1.4758,
      "step": 2580
    },
    {
      "epoch": 0.7968617798630875,
      "grad_norm": 2.3285350799560547,
      "learning_rate": 2.08928965343659e-06,
      "loss": 1.4898,
      "step": 2590
    },
    {
      "epoch": 0.7999384662718253,
      "grad_norm": 2.425253391265869,
      "learning_rate": 2.0286749277707783e-06,
      "loss": 1.3945,
      "step": 2600
    },
    {
      "epoch": 0.803015152680563,
      "grad_norm": 1.9754968881607056,
      "learning_rate": 1.9688530786269854e-06,
      "loss": 1.4816,
      "step": 2610
    },
    {
      "epoch": 0.8060918390893008,
      "grad_norm": 1.7054038047790527,
      "learning_rate": 1.9098300562505266e-06,
      "loss": 1.4602,
      "step": 2620
    },
    {
      "epoch": 0.8091685254980386,
      "grad_norm": 1.897717833518982,
      "learning_rate": 1.8516117314305526e-06,
      "loss": 1.4237,
      "step": 2630
    },
    {
      "epoch": 0.8122452119067765,
      "grad_norm": 1.718672513961792,
      "learning_rate": 1.7942038949160857e-06,
      "loss": 1.5085,
      "step": 2640
    },
    {
      "epoch": 0.8153218983155142,
      "grad_norm": 2.0416133403778076,
      "learning_rate": 1.7376122568400522e-06,
      "loss": 1.4337,
      "step": 2650
    },
    {
      "epoch": 0.818398584724252,
      "grad_norm": 1.979547142982483,
      "learning_rate": 1.6818424461513116e-06,
      "loss": 1.4515,
      "step": 2660
    },
    {
      "epoch": 0.8214752711329898,
      "grad_norm": 1.6698029041290283,
      "learning_rate": 1.6269000100547682e-06,
      "loss": 1.518,
      "step": 2670
    },
    {
      "epoch": 0.8245519575417276,
      "grad_norm": 2.3249824047088623,
      "learning_rate": 1.5727904134596084e-06,
      "loss": 1.3982,
      "step": 2680
    },
    {
      "epoch": 0.8276286439504653,
      "grad_norm": 2.0947065353393555,
      "learning_rate": 1.5195190384357405e-06,
      "loss": 1.477,
      "step": 2690
    },
    {
      "epoch": 0.8307053303592031,
      "grad_norm": 1.8471100330352783,
      "learning_rate": 1.467091183678444e-06,
      "loss": 1.4037,
      "step": 2700
    },
    {
      "epoch": 0.833782016767941,
      "grad_norm": 1.7797983884811401,
      "learning_rate": 1.4155120639813392e-06,
      "loss": 1.4384,
      "step": 2710
    },
    {
      "epoch": 0.8368587031766788,
      "grad_norm": 1.96023690700531,
      "learning_rate": 1.364786809717692e-06,
      "loss": 1.4676,
      "step": 2720
    },
    {
      "epoch": 0.8399353895854165,
      "grad_norm": 1.7855671644210815,
      "learning_rate": 1.3149204663301118e-06,
      "loss": 1.4343,
      "step": 2730
    },
    {
      "epoch": 0.8430120759941543,
      "grad_norm": 1.798910140991211,
      "learning_rate": 1.2659179938287035e-06,
      "loss": 1.401,
      "step": 2740
    },
    {
      "epoch": 0.8460887624028921,
      "grad_norm": 1.5057716369628906,
      "learning_rate": 1.2177842662977136e-06,
      "loss": 1.4376,
      "step": 2750
    },
    {
      "epoch": 0.8491654488116299,
      "grad_norm": 3.6766085624694824,
      "learning_rate": 1.1705240714107301e-06,
      "loss": 1.3905,
      "step": 2760
    },
    {
      "epoch": 0.8522421352203676,
      "grad_norm": 1.8102703094482422,
      "learning_rate": 1.124142109954459e-06,
      "loss": 1.4921,
      "step": 2770
    },
    {
      "epoch": 0.8553188216291054,
      "grad_norm": 1.4987050294876099,
      "learning_rate": 1.0786429953611665e-06,
      "loss": 1.4722,
      "step": 2780
    },
    {
      "epoch": 0.8583955080378433,
      "grad_norm": 1.8079637289047241,
      "learning_rate": 1.034031253249792e-06,
      "loss": 1.4373,
      "step": 2790
    },
    {
      "epoch": 0.8614721944465811,
      "grad_norm": 2.5194859504699707,
      "learning_rate": 9.903113209758098e-07,
      "loss": 1.4843,
      "step": 2800
    },
    {
      "epoch": 0.8645488808553188,
      "grad_norm": 2.4704389572143555,
      "learning_rate": 9.474875471898526e-07,
      "loss": 1.4221,
      "step": 2810
    },
    {
      "epoch": 0.8676255672640566,
      "grad_norm": 2.581787586212158,
      "learning_rate": 9.055641914051783e-07,
      "loss": 1.4454,
      "step": 2820
    },
    {
      "epoch": 0.8707022536727944,
      "grad_norm": 2.1384406089782715,
      "learning_rate": 8.645454235739903e-07,
      "loss": 1.4461,
      "step": 2830
    },
    {
      "epoch": 0.8737789400815322,
      "grad_norm": 2.230163097381592,
      "learning_rate": 8.24435323672661e-07,
      "loss": 1.4412,
      "step": 2840
    },
    {
      "epoch": 0.8768556264902699,
      "grad_norm": 1.6834113597869873,
      "learning_rate": 7.852378812959227e-07,
      "loss": 1.4828,
      "step": 2850
    },
    {
      "epoch": 0.8799323128990078,
      "grad_norm": 2.344066619873047,
      "learning_rate": 7.46956995260033e-07,
      "loss": 1.4492,
      "step": 2860
    },
    {
      "epoch": 0.8830089993077456,
      "grad_norm": 1.9996334314346313,
      "learning_rate": 7.095964732149741e-07,
      "loss": 1.4004,
      "step": 2870
    },
    {
      "epoch": 0.8860856857164834,
      "grad_norm": 3.037078857421875,
      "learning_rate": 6.731600312657238e-07,
      "loss": 1.5026,
      "step": 2880
    },
    {
      "epoch": 0.8891623721252211,
      "grad_norm": 2.3848016262054443,
      "learning_rate": 6.37651293602628e-07,
      "loss": 1.4009,
      "step": 2890
    },
    {
      "epoch": 0.8922390585339589,
      "grad_norm": 2.2819950580596924,
      "learning_rate": 6.030737921409169e-07,
      "loss": 1.4501,
      "step": 2900
    },
    {
      "epoch": 0.8953157449426967,
      "grad_norm": 2.319035530090332,
      "learning_rate": 5.694309661693942e-07,
      "loss": 1.4142,
      "step": 2910
    },
    {
      "epoch": 0.8983924313514345,
      "grad_norm": 2.1951498985290527,
      "learning_rate": 5.367261620083575e-07,
      "loss": 1.4725,
      "step": 2920
    },
    {
      "epoch": 0.9014691177601722,
      "grad_norm": 2.1686959266662598,
      "learning_rate": 5.049626326767366e-07,
      "loss": 1.4653,
      "step": 2930
    },
    {
      "epoch": 0.9045458041689101,
      "grad_norm": 2.139129400253296,
      "learning_rate": 4.7414353756853773e-07,
      "loss": 1.4937,
      "step": 2940
    },
    {
      "epoch": 0.9076224905776479,
      "grad_norm": 1.9565069675445557,
      "learning_rate": 4.4427194213859216e-07,
      "loss": 1.4518,
      "step": 2950
    },
    {
      "epoch": 0.9106991769863857,
      "grad_norm": 2.10117506980896,
      "learning_rate": 4.1535081759764286e-07,
      "loss": 1.44,
      "step": 2960
    },
    {
      "epoch": 0.9137758633951234,
      "grad_norm": 1.8772257566452026,
      "learning_rate": 3.8738304061681107e-07,
      "loss": 1.4692,
      "step": 2970
    },
    {
      "epoch": 0.9168525498038612,
      "grad_norm": 2.9377400875091553,
      "learning_rate": 3.603713930414676e-07,
      "loss": 1.4208,
      "step": 2980
    },
    {
      "epoch": 0.919929236212599,
      "grad_norm": 1.6773600578308105,
      "learning_rate": 3.3431856161452835e-07,
      "loss": 1.4033,
      "step": 2990
    },
    {
      "epoch": 0.9230059226213368,
      "grad_norm": 2.3917605876922607,
      "learning_rate": 3.0922713770922155e-07,
      "loss": 1.4564,
      "step": 3000
    },
    {
      "epoch": 0.9260826090300746,
      "grad_norm": 2.5713582038879395,
      "learning_rate": 2.8509961707132496e-07,
      "loss": 1.4087,
      "step": 3010
    },
    {
      "epoch": 0.9291592954388124,
      "grad_norm": 2.368323564529419,
      "learning_rate": 2.6193839957093683e-07,
      "loss": 1.4055,
      "step": 3020
    },
    {
      "epoch": 0.9322359818475502,
      "grad_norm": 1.9347461462020874,
      "learning_rate": 2.3974578896375555e-07,
      "loss": 1.4018,
      "step": 3030
    },
    {
      "epoch": 0.935312668256288,
      "grad_norm": 1.500869870185852,
      "learning_rate": 2.1852399266194312e-07,
      "loss": 1.4623,
      "step": 3040
    },
    {
      "epoch": 0.9383893546650258,
      "grad_norm": 1.6664708852767944,
      "learning_rate": 1.9827512151456175e-07,
      "loss": 1.4421,
      "step": 3050
    },
    {
      "epoch": 0.9414660410737635,
      "grad_norm": 1.7920416593551636,
      "learning_rate": 1.7900118959761181e-07,
      "loss": 1.4181,
      "step": 3060
    },
    {
      "epoch": 0.9445427274825013,
      "grad_norm": 2.176259994506836,
      "learning_rate": 1.6070411401370335e-07,
      "loss": 1.4256,
      "step": 3070
    },
    {
      "epoch": 0.9476194138912392,
      "grad_norm": 2.073683738708496,
      "learning_rate": 1.4338571470137063e-07,
      "loss": 1.5099,
      "step": 3080
    },
    {
      "epoch": 0.950696100299977,
      "grad_norm": 1.9588308334350586,
      "learning_rate": 1.2704771425404382e-07,
      "loss": 1.4055,
      "step": 3090
    },
    {
      "epoch": 0.9537727867087147,
      "grad_norm": 2.2666244506835938,
      "learning_rate": 1.1169173774871478e-07,
      "loss": 1.4601,
      "step": 3100
    },
    {
      "epoch": 0.9568494731174525,
      "grad_norm": 1.9847588539123535,
      "learning_rate": 9.731931258429638e-08,
      "loss": 1.4055,
      "step": 3110
    },
    {
      "epoch": 0.9599261595261903,
      "grad_norm": 1.6538118124008179,
      "learning_rate": 8.393186832969746e-08,
      "loss": 1.4146,
      "step": 3120
    },
    {
      "epoch": 0.9630028459349281,
      "grad_norm": 2.378875732421875,
      "learning_rate": 7.153073658162646e-08,
      "loss": 1.4321,
      "step": 3130
    },
    {
      "epoch": 0.9660795323436658,
      "grad_norm": 2.2889790534973145,
      "learning_rate": 6.011715083214742e-08,
      "loss": 1.486,
      "step": 3140
    },
    {
      "epoch": 0.9691562187524037,
      "grad_norm": 1.8135160207748413,
      "learning_rate": 4.9692246345985905e-08,
      "loss": 1.4636,
      "step": 3150
    },
    {
      "epoch": 0.9722329051611415,
      "grad_norm": 2.164019823074341,
      "learning_rate": 4.025706004760932e-08,
      "loss": 1.4887,
      "step": 3160
    },
    {
      "epoch": 0.9753095915698793,
      "grad_norm": 2.051666498184204,
      "learning_rate": 3.181253041809052e-08,
      "loss": 1.5116,
      "step": 3170
    },
    {
      "epoch": 0.978386277978617,
      "grad_norm": 1.506913423538208,
      "learning_rate": 2.4359497401758026e-08,
      "loss": 1.462,
      "step": 3180
    },
    {
      "epoch": 0.9814629643873548,
      "grad_norm": 2.365898370742798,
      "learning_rate": 1.7898702322648453e-08,
      "loss": 1.4387,
      "step": 3190
    },
    {
      "epoch": 0.9845396507960926,
      "grad_norm": 2.0541090965270996,
      "learning_rate": 1.2430787810776556e-08,
      "loss": 1.4105,
      "step": 3200
    },
    {
      "epoch": 0.9876163372048304,
      "grad_norm": 1.6732237339019775,
      "learning_rate": 7.956297738207496e-09,
      "loss": 1.5168,
      "step": 3210
    },
    {
      "epoch": 0.9906930236135681,
      "grad_norm": 2.214223861694336,
      "learning_rate": 4.475677164966774e-09,
      "loss": 1.4024,
      "step": 3220
    },
    {
      "epoch": 0.993769710022306,
      "grad_norm": 1.6355136632919312,
      "learning_rate": 1.9892722947645328e-09,
      "loss": 1.4314,
      "step": 3230
    },
    {
      "epoch": 0.9968463964310438,
      "grad_norm": 2.0808331966400146,
      "learning_rate": 4.973304405697654e-10,
      "loss": 1.465,
      "step": 3240
    },
    {
      "epoch": 0.9999230828397816,
      "grad_norm": 1.5428138971328735,
      "learning_rate": 0.0,
      "loss": 1.4615,
      "step": 3250
    }
  ],
  "logging_steps": 10,
  "max_steps": 3250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.66264785862656e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
